const express = require('express');
const puppeteer = require('puppeteer');
const archiver = require('archiver');
const fs = require('fs').promises;
const path = require('path');
const { URL } = require('url');

const app = express();
const PORT = process.env.PORT || 3000;

app.use(express.json());
app.use(express.static('.'));

// Ð¥Ñ€Ð°Ð½Ð¸Ð»Ð¸Ñ‰Ðµ Ð·Ð°Ð´Ð°Ñ‡
const jobs = {};

app.get('/', (req, res) => {
  res.sendFile(__dirname + '/index.html');
});

// Ð—Ð°Ð¿ÑƒÑÐº Ð½Ð¾Ð²Ð¾Ð¹ Ð·Ð°Ð´Ð°Ñ‡Ð¸
app.post('/start', async (req, res) => {
  const startUrl = req.body.url?.trim();
  const maxDepthInput = req.body.depth || '2';
  const maxDepth = Math.min(3, Math.max(1, parseInt(maxDepthInput)));

  if (!startUrl || !startUrl.startsWith('http')) {
    return res.status(400).send('âŒ Ð£ÐºÐ°Ð¶Ð¸Ñ‚Ðµ ÐºÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð½Ñ‹Ð¹ URL (Ð½Ð°Ñ‡Ð¸Ð½Ð°ÐµÑ‚ÑÑ Ñ http)');
  }

  const jobId = Date.now().toString(36);
  jobs[jobId] = { logs: ['ðŸš€ Ð—Ð°Ð´Ð°Ñ‡Ð° Ð·Ð°Ð¿ÑƒÑ‰ÐµÐ½Ð°...'], done: false, zipPath: null };
  res.json({ jobId });

  // Ð—Ð°Ð¿ÑƒÑÐºÐ°ÐµÐ¼ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÑƒ Ð² Ñ„Ð¾Ð½Ðµ
  (async () => {
    try {
      await processSite(jobId, startUrl, maxDepth);
      jobs[jobId].done = true;
    } catch (err) {
      jobs[jobId].logs.push(`âŒ Ð’Ð½ÑƒÑ‚Ñ€ÐµÐ½Ð½ÑÑ Ð¾ÑˆÐ¸Ð±ÐºÐ°: ${err.message}`);
      jobs[jobId].done = true;
    }
  })();
});

// ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ðµ ÑÑ‚Ð°Ñ‚ÑƒÑÐ° Ð·Ð°Ð´Ð°Ñ‡Ð¸
app.get('/status/:jobId', (req, res) => {
  const job = jobs[req.params.jobId];
  if (!job) {
    return res.status(404).json({ logs: ['âš ï¸ Ð—Ð°Ð´Ð°Ñ‡Ð° Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð°'], done: true });
  }
  res.json({ logs: job.logs, done: job.done });
});

// Ð¡ÐºÐ°Ñ‡Ð¸Ð²Ð°Ð½Ð¸Ðµ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð°
app.get('/download/:jobId', async (req, res) => {
  const job = jobs[req.params.jobId];
  if (!job || !job.done || !job.zipPath) {
    return res.status(404).send('Ð—Ð°Ð´Ð°Ñ‡Ð° Ð½Ðµ Ð³Ð¾Ñ‚Ð¾Ð²Ð° Ð¸Ð»Ð¸ Ð½Ðµ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÐµÑ‚');
  }
  res.download(job.zipPath, 'site-export.zip', async () => {
    // ÐžÐ¿Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾: ÑƒÐ´Ð°Ð»Ð¸ Ð¿Ð¾ÑÐ»Ðµ Ð¾Ñ‚Ð´Ð°Ñ‡Ð¸
    try {
      await fs.unlink(job.zipPath);
      await fs.rm(path.dirname(job.zipPath), { recursive: true, force: true });
    } catch (e) {}
    delete jobs[req.params.jobId];
  });
});

// ÐžÑÐ½Ð¾Ð²Ð½Ð°Ñ Ð»Ð¾Ð³Ð¸ÐºÐ° Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸
async function processSite(jobId, startUrl, maxDepth) {
  const job = jobs[jobId];
  const normalizedUrl = new URL(startUrl).href;
  const baseUrl = new URL(normalizedUrl).origin;
  const visited = new Set();
  const queue = [{ url: normalizedUrl, depth: 0 }];
  const pdfDir = path.join(__dirname, `pdfs_${jobId}`);
  const zipPath = path.join(__dirname, `site-export_${jobId}.zip`);

  job.logs.push(`ðŸŒ Ð‘Ð°Ð·Ð¾Ð²Ñ‹Ð¹ URL: ${baseUrl}`);
  job.logs.push(`ðŸ§­ Ð“Ð»ÑƒÐ±Ð¸Ð½Ð° Ð¾Ð±Ñ…Ð¾Ð´Ð°: ${maxDepth}`);

  try {
    await fs.rm(pdfDir, { recursive: true, force: true });
    await fs.mkdir(pdfDir, { recursive: true });

    const browser = await puppeteer.launch({
      headless: true,
      args: ['--no-sandbox', '--disable-setuid-sandbox', '--disable-dev-shm-usage']
    });
    const page = await browser.newPage();
    await page.setViewport({ width: 1280, height: 800 });

    while (queue.length > 0) {
      const { url, depth } = queue.shift();
      if (visited.has(url)) continue;
      if (!url.startsWith(baseUrl)) continue;

      visited.add(url);
      job.logs.push(`ðŸ“¥ [${depth}/${maxDepth}] ${url}`);

      try {
        await page.goto(url, { waitUntil: 'networkidle2', timeout: 15000 });

        let name = url
          .replace(baseUrl, '')
          .replace(/^\/|\/$/g, '')
          .replace(/\//g, '_')
          .replace(/[^a-z0-9_-]/gi, '_') || 'index';

        const pdfPath = path.join(pdfDir, `${name}.pdf`);
        await page.pdf({ path: pdfPath, format: 'A4', printBackground: true });
        job.logs.push(`âœ… Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¾: ${name}.pdf`);

        if (depth < maxDepth) {
          const links = await page.evaluate(() =>
            Array.from(document.querySelectorAll('a[href]'))
              .map(a => a.getAttribute('href'))
              .filter(href => href && !href.startsWith('#') && href.startsWith('/'))
          );
          for (const href of links) {
            try {
              const fullUrl = new URL(href, baseUrl).href;
              if (!visited.has(fullUrl)) {
                queue.push({ url: fullUrl, depth: depth + 1 });
              }
            } catch (e) {
              job.logs.push(`âš ï¸ ÐÐµÐºÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð½Ð°Ñ ÑÑÑ‹Ð»ÐºÐ°: ${href}`);
            }
          }
        }
      } catch (e) {
        job.logs.push(`âš ï¸ ÐŸÑ€Ð¾Ð¿ÑƒÑ‰ÐµÐ½Ð¾: ${url} â€” ${e.message}`);
      }
    }

    await browser.close();

    // Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ ZIP
    job.logs.push('ðŸ“¦ Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ ZIP-Ð°Ñ€Ñ…Ð¸Ð²Ð°...');
    const zipStream = fs.createWriteStream(zipPath);
    const archive = archiver('zip', { zlib: { level: 6 } });
    archive.pipe(zipStream);
    for (const file of await fs.readdir(pdfDir)) {
      archive.file(path.join(pdfDir, file), { name: file });
    }
    await archive.finalize();
    await new Promise(resolve => zipStream.on('close', resolve));

    job.zipPath = zipPath;
    job.logs.push('âœ… ZIP Ð³Ð¾Ñ‚Ð¾Ð².');
  } catch (err) {
    job.logs.push(`ðŸ’¥ ÐšÑ€Ð¸Ñ‚Ð¸Ñ‡ÐµÑÐºÐ°Ñ Ð¾ÑˆÐ¸Ð±ÐºÐ°: ${err.message}`);
    throw err;
  }
}

app.listen(PORT, () => {
  console.log(`âœ… Ð¡ÐµÑ€Ð²ÐµÑ€ Ð·Ð°Ð¿ÑƒÑ‰ÐµÐ½: http://localhost:${PORT}`);
});
